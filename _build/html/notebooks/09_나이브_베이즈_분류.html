
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>9장 나이브 베이즈 분류 &#8212; 데이터과학의 첫걸음</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'notebooks/09_나이브_베이즈_분류';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="10장 변수선택 및 정규화" href="10_%EB%B3%80%EC%88%98%EC%84%A0%ED%83%9D_%EB%B0%8F_%EC%A0%95%EA%B7%9C%ED%99%94.html" />
    <link rel="prev" title="8장 KNN 분류" href="08_KNN%EC%9D%84_%EC%9D%B4%EC%9A%A9%ED%95%9C_%EB%B6%84%EB%A5%98.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="00_%EC%84%9C%EB%AC%B8.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="데이터과학의 첫걸음 - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="데이터과학의 첫걸음 - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="00_%EC%84%9C%EB%AC%B8.html">
                    데이터과학의 첫걸음: 시작하며
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01_%ED%8C%8C%EC%9D%B4%EC%8D%AC_%EC%8B%9C%EC%9E%91%ED%95%98%EA%B8%B0.html">1장 파이썬 시작하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_%ED%8C%8C%EC%9D%B4%EC%8D%AC_%EC%BD%94%EB%94%A9_%EA%B8%B0%EC%B4%88.html">2장 파이썬 코딩 기초</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_%EC%9D%80%ED%96%89_%ED%85%94%EB%A0%88%EB%A7%88%EC%BC%80%ED%8C%85_%EB%8D%B0%EC%9D%B4%ED%84%B0_%EB%B6%84%EC%84%9D.html">3장 은행 텔레마케팅 데이터 분석</a></li>
<li class="toctree-l1"><a class="reference internal" href="04_%EC%A0%84%EC%9E%90%EC%83%81%EA%B1%B0%EB%9E%98_%EB%8D%B0%EC%9D%B4%ED%84%B0_%EB%B6%84%EC%84%9D.html">4장 전자상거래 데이터 분석</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_%ED%86%B5%EA%B3%84%EC%A0%81_%ED%95%99%EC%8A%B5.html">5장 통계적 학습</a></li>
<li class="toctree-l1"><a class="reference internal" href="06_%EC%84%A0%ED%98%95_%ED%9A%8C%EA%B7%80.html">6장 선형 회귀</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1_%ED%9A%8C%EA%B7%80%EB%A5%BC_%EC%9D%B4%EC%9A%A9%ED%95%9C_%EB%B6%84%EB%A5%98.html">7장 로지스틱 회귀를 이용한 분류</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_KNN%EC%9D%84_%EC%9D%B4%EC%9A%A9%ED%95%9C_%EB%B6%84%EB%A5%98.html">8장 KNN 분류</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">9장 나이브 베이즈 분류</a></li>
<li class="toctree-l1"><a class="reference internal" href="10_%EB%B3%80%EC%88%98%EC%84%A0%ED%83%9D_%EB%B0%8F_%EC%A0%95%EA%B7%9C%ED%99%94.html">10장 변수선택 및 정규화</a></li>
<li class="toctree-l1"><a class="reference internal" href="11_%EC%B6%94%EC%B2%9C_%EC%8B%9C%EC%8A%A4%ED%85%9C.html">11장 추천 시스템</a></li>
<li class="toctree-l1"><a class="reference internal" href="12_%EA%B2%B0%EC%A0%95_%ED%8A%B8%EB%A6%AC_%EB%A7%8C%EB%93%A4%EA%B8%B0.html">12장 결정 트리 만들기</a></li>
<li class="toctree-l1"><a class="reference internal" href="13_%ED%8A%B8%EB%A6%AC_%EA%B8%B0%EB%B0%98_%EB%AA%A8%ED%98%95.html">13장 트리 기반 모형</a></li>
<li class="toctree-l1"><a class="reference internal" href="14_%EC%84%9C%ED%8F%AC%ED%8A%B8_%EB%B2%A1%ED%84%B0_%EB%A8%B8%EC%8B%A0.html">14장 서포트 벡터 머신</a></li>
<li class="toctree-l1"><a class="reference internal" href="15_%EB%94%A5%EB%9F%AC%EB%8B%9D_%EC%9E%85%EB%AC%B8.html">15장 딥러닝 입문</a></li>
<li class="toctree-l1"><a class="reference internal" href="16_%EB%B9%84%EC%A7%80%EB%8F%84%ED%95%99%EC%8A%B5%EA%B3%BC_%EC%A3%BC%EC%84%B1%EB%B6%84_%EB%B6%84%EC%84%9D.html">16장 비지도학습과 주성분 분석</a></li>
<li class="toctree-l1"><a class="reference internal" href="17_%EC%83%9D%EC%A1%B4%EB%B6%84%EC%84%9D.html">17장 생존분석</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/pilsunchoi/book1" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/pilsunchoi/book1/issues/new?title=Issue%20on%20page%20%2Fnotebooks/09_나이브_베이즈_분류.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/notebooks/09_나이브_베이즈_분류.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>9장 나이브 베이즈 분류</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">9.1 스팸 필터링 예제: 확률론적 접근</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">스팸 필터링 예제 1</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">스팸 필터링 예제 2</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">조건부확률과 베이즈 정리</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">9.2 나이브 베이즈 분류 예제</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">스팸 메일 분류 예제</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#naive">순진한(naive) 가정</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">9.3 나이브 베이즈 알고리즘</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">확률 모델을 이용한 분류기</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">사전적 확률 추정</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">가능도 추정</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">9.4 나이브 베이즈 실습</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">주식시장 데이터</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">나이브 베이즈 분류</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id15">모델별 정확도 비교</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">타이타닉 데이터</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id17">성별에 의한 생존 여부 예측</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id18">나이 및 운임에 의한 생존 여부 예측</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#chatgpt-q-a">ChatGPT Q&amp;A</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id19">ChatGPT Q&amp;A</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="id1">
<h1>9장 나이브 베이즈 분류<a class="headerlink" href="#id1" title="Link to this heading">#</a></h1>
<p><strong>나이브 베이즈</strong>(naive Bayes)는 분류(classification)에 사용되는 아주 간단한 기법이다.  베이즈 정리를 기반으로 하되, 예측변수 간에 강력한(순진한) 독립성(independence) 가정을 적용하는 간단한 확률론적 분류기(probabilistic classifier)이다.(wikipedia, “<a class="reference external" href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier">Naive Bayes classifier</a>”.)</p>
<section id="id2">
<h2>9.1 스팸 필터링 예제: 확률론적 접근<a class="headerlink" href="#id2" title="Link to this heading">#</a></h2>
<section id="id3">
<h3>스팸 필터링 예제 1<a class="headerlink" href="#id3" title="Link to this heading">#</a></h3>
<p>정크메일(스팸) 필터링과 관련된 아주 간단한 예제를 생각해보자. 100개의 이메일이 있다. 아래 표에서 보듯이 1번 이메일은 정상적인 이메일, 즉 햄(ham)이고, 2번 이메일은 스팸(spam)이고, 이런 식으로 100개 이메일의 클래스(class)를 구분해 놓았다. 이 상태에서 우리의 과제는 아래 표의 맨 아래에 0번으로 표시된 이메일이 스팸인지, 햄인지를 분류하는 것이다.</p>
<p>이 책에서 그동안 사용해 온 용어로 표현하면, 아래 데이터세트에는 100개의 훈련 관측이 있고, 예측변수 또는 특성(feature)은 없으며, 반응(response) <span class="math notranslate nohighlight">\(Y\)</span>는 범주(<strong>클래스</strong>)로 된 이항(binary) 변수이다. 그리고 맨 아래 0번 관측은 테스트 관측에 해당한다.</p>
<p><img alt="스팸 필터링 예제 1" src="https://raw.githubusercontent.com/pilsunchoi/images/main/9-1.png" /></p>
<p><strong>확률론적 접근</strong></p>
<p>이 문제에 대한 <strong>확률론적</strong>(probabilistic) 접근은 테스트 관측이 스팸 및 햄일 <strong>확률을 추정해 비교</strong>해보는 것이다.</p>
<p>100개의 훈련 관측을 분류한 결과가 아래 다이어그램과 같다고 가정해보자. 즉, 100개의 이메일 중 스팸은 20%이고, 햄은 80%이다. <strong>이 정보만으로</strong> 테스트 관측의 클래스를 <strong>예측</strong>한다면, 스팸 확률은 0.2이고, 햄 확률은 0.8이라고 예측하는 것이 최선일 것이다. 이를 기호로 표시하면 <span class="math notranslate nohighlight">\(p\rm(spam)=0.2\)</span>, <span class="math notranslate nohighlight">\(p\rm(ham)=0.8\)</span>이다.</p>
<p>만약 스팸과 햄 중 어느 쪽 확률이 크냐에 따라 클래스를 분류하기로 <strong>결정 규칙</strong>(decision rule)을 세운다면, 이런 경우 테스트 관측(0번 관측)은 햄으로 분류된다.</p>
<p><img alt="벤 다이어그램" src="https://raw.githubusercontent.com/pilsunchoi/images/main/9-2.png" /></p>
<ul class="simple">
<li><p>그림 출처: Brett Lantz (2015), Machine Learing With R, 4장.</p></li>
</ul>
</section>
<section id="id4">
<h3>스팸 필터링 예제 2<a class="headerlink" href="#id4" title="Link to this heading">#</a></h3>
<p>또 다른 정크메일(스팸) 필터링 예제를 생각해보자. 이번에도 100개의 이메일이 있다. 아래 표에서 보듯이 이번에는 추가적인 정보가 있는데, 이메일 메시지에 “Viagra”라는 단어가 들어가 있는지 여부가 Yes/No로 표시돼 있다. 이 <span class="math notranslate nohighlight">\(X\)</span> 변수는 반응(<span class="math notranslate nohighlight">\(Y\)</span>)의 클래스를 예측하는 데 사용되는 예측변수에 해당한다.</p>
<p><img alt="스팸 필터링 예제 2" src="https://raw.githubusercontent.com/pilsunchoi/images/main/9-3.png" /></p>
<p>이 상태에서 우리의 과제는 표의 맨 아래에 0번으로 표시된 이메일이 스팸인지, 햄인지를 분류하는 것이다. 0번 관측과 관련하여 앞의 예제와 다른 점은 우리에게 추가적인 정보가 있다는 것인데, 0번 이메일 메시지에 Viagra 단어가 들어있다는 점이다.</p>
<p>통계학에서는 이런 확률을 <strong>조건부확률</strong>(conditional probability)이라고 부른다. 이메일 메시지에 Viagra 단어가 들어있다는 <strong>조건</strong>(<span class="math notranslate nohighlight">\(\rm Viagra=Yes\)</span>) <strong>하에서</strong> <code class="docutils literal notranslate"><span class="pre">spam</span></code>과 <code class="docutils literal notranslate"><span class="pre">ham</span></code>의 확률을 따지는 것으로 이를 각각 <span class="math notranslate nohighlight">\(p\rm(spam \mid Yes)\)</span>와 <span class="math notranslate nohighlight">\(p\rm(ham \mid Yes)\)</span>로 표기한다.</p>
<p>사실 우리 경우에는 <span class="math notranslate nohighlight">\(p\rm(spam \mid Yes)\)</span>와 <span class="math notranslate nohighlight">\(p\rm(ham \mid Yes)\)</span> 중 어느 하나만 추정하면 된다. 왜냐하면 클래스가 <code class="docutils literal notranslate"><span class="pre">spam</span></code>과 <code class="docutils literal notranslate"><span class="pre">ham</span></code> 두 개밖에 없기 때문에 어느 한 쪽의 확률을 구하면, 다른 쪽 확률은 1에서 해당 확률을 빼면 되기 때문이다. 여기에서는 <span class="math notranslate nohighlight">\(p\rm(spam \mid Yes)\)</span>를 추정해보기로 하자.</p>
<p><strong><code class="docutils literal notranslate"><span class="pre">spam</span></code>/<code class="docutils literal notranslate"><span class="pre">ham</span></code>과 <code class="docutils literal notranslate"><span class="pre">Viagra</span></code>의 관계</strong></p>
<p>훈련 세트에 들어있는 100개의 이메일을 분석한 결과, 클래스(<code class="docutils literal notranslate"><span class="pre">spam</span></code>/<code class="docutils literal notranslate"><span class="pre">ham</span></code>)와 <code class="docutils literal notranslate"><span class="pre">Viagra</span></code>의 관계가 아래 표 9.1과 같다고 해보자. 이것은 Viagra 단어가 들어있는 경우와 그렇지 않은 각 경우에 대해 <code class="docutils literal notranslate"><span class="pre">spam</span></code>/<code class="docutils literal notranslate"><span class="pre">ham</span></code>이 몇 개인지 빈도수(frequency)를 보여준다. 소위 <strong>분할표</strong>(contingency table)이다. 100개의 이메일 중 스팸은 20%이고, 햄은 80%라는 점은 앞의 예제와 동일하다.</p>
<p>이 표의 정보를 이용하면 <span class="math notranslate nohighlight">\(p\rm(spam \mid Yes)\)</span>를 쉽게 구할 수 있다. 조건부확률은 말 그대로 해당 조건 내에서 확률을 따지는 것이기 때문에 여기서는 <strong>Viagra 단어가 들어있는 이메일 중에서 스팸이 발생할 확률</strong>을 계산하면 된다. 아래 분할표에 따르면 전체 이메일 중 Viagra 단어가 들어있는 것은 5개이고, 이 중에서 4개(80%)가 스팸이다. 따라서 <span class="math notranslate nohighlight">\(p\rm(spam \mid Yes)=0.8\)</span>이고, <span class="math notranslate nohighlight">\(p{\rm(ham \mid Yes)}=1-p{\rm(spam \mid Yes)}=0.2\)</span>가 된다. 스팸일 확률이 더 크므로 0번 관측은 정크메일로 분류된다. 앞의 예제 1과는 반대의 결과이다.</p>
<p><strong>표 9.1.</strong> 스팸 메일 필터링 예제 2.</p>
<p><img alt="스팸 필터링 예제 2 표" src="https://raw.githubusercontent.com/pilsunchoi/images/main/9-4.png" /></p>
<ul class="simple">
<li><p>표 출처: Brett Lantz (2015), Machine Learing With R, 4장.</p></li>
</ul>
</section>
<section id="id5">
<h3>조건부확률과 베이즈 정리<a class="headerlink" href="#id5" title="Link to this heading">#</a></h3>
<p>나이브 베이즈를 이해하기 위해서는 조건부확률에 대해 좀 더 알아야 한다. 조건부확률은 다음과 같이 정의된다.</p>
<div class="math notranslate nohighlight">
\[p(y\mid x)={\frac {p(y, x)}{p(x)}} \tag{9.1}\]</div>
<p>여기에서 <span class="math notranslate nohighlight">\(p(y, x)\)</span>는 <span class="math notranslate nohighlight">\(y\)</span>와 <span class="math notranslate nohighlight">\(x\)</span>가 동시에 발생할 확률로서 <strong>결합확률</strong>(joint probability)이라고 부른다.</p>
<p>앞에서 예로 다루었던 <span class="math notranslate nohighlight">\(p\rm(spam\mid Yes)\)</span>를 위의 조건부확률 정의에 따라 계산해보자. 위 표 9.1을 보면 <span class="math notranslate nohighlight">\(p\rm (Yes)=5/100\)</span>이고, <span class="math notranslate nohighlight">\(p\rm(spam, Yes)=4/100\)</span>이다. 따라서 조건부확률의 정의(식 9.1)에 따라 계산하면 0.8로서 앞에서와 동일한 결과를 얻는다.</p>
<div class="math notranslate nohighlight">
\[p{\rm(spam\mid Yes)}={\frac {p{\rm(spam, Yes)}}{p{\rm(Yes)}}}=\frac{4/100}{5/100}=\frac{4}{5}=0.8\]</div>
<p>이제 베이즈 정리를 도출해보자. 베이즈 정리는 조건부확률의 정의에서 출발하는데, 위 식 9.1에 있는 결합확률 <span class="math notranslate nohighlight">\(p(y, x)\)</span>의 경우, 괄호안 두 개 변수의 순서를 바꿔도 내용상 전혀 변화가 없다는 점에서 <span class="math notranslate nohighlight">\(p(y, x)=p(x, y)\)</span>가 성립한다. 또한 <span class="math notranslate nohighlight">\(p(x, y)\)</span>는 위 조건부확률의 정의에 의해 다음과 같이 쓸 수 있다.</p>
<div class="math notranslate nohighlight">
\[p(x\mid y)={\frac {p(x, y)}{p(y)}} \Rightarrow \ p(x, y)=p(y)\ p(x\mid y) \tag{9.2}\]</div>
<p>따라서 식 9.1과 9.2에 의해 다음의 <strong>베이즈 정리</strong>(Bayes’ theorem)가 도출된다.</p>
<div class="math notranslate nohighlight">
\[p(y\mid x)= \frac{p(y)\ p(x\mid y)}{ p(x)} \tag{9.3}\]</div>
<p>베이즈 정리의 의미를 생각해보기 위해 앞의 스팸 필터링 예에 베이즈 정리를 적용하면 다음과 같다.</p>
<div class="math notranslate nohighlight">
\[p{\rm(spam\mid Yes)}=\frac {p{\rm(spam)}\ p{\rm(Yes\mid spam)}}{p{\rm(Yes)}} \tag{9.4}\]</div>
<p>베이즈 정리의 <strong>의미</strong>는 다음과 같다.</p>
<div class="math notranslate nohighlight">
\[{\text{posterior}}={\frac {{\text{prior}}\times {\text{likelihood}}}{\text{evidence}}}\]</div>
<p>식 9.4에서 <span class="math notranslate nohighlight">\(p{\rm(spam)}\)</span>을 <strong>사전적 확률</strong>(prior probability)이라고 한다. 다른 정보가 없는 상태에서 어떤 이메일이 스팸일 확률이다. 그런데 이제 이메일 메시지에 들어있는 단어들을 살펴 볼 수 있게 됨으로써 Viagra 단어의 존재 유무에 대한 정보를 얻었다고 해보자. 이처럼 Viagra 단어의 존재 유무에 대한 추가 정보가 있는 상황에서 이메일이 스팸일 확률, 즉 조건부확률 <span class="math notranslate nohighlight">\(p{\rm(spam\mid Yes)}\)</span>를 <strong>사후적 확률</strong>(posterior probability)이라고 부른다. 그것은 사전적 확률을 업데이트하는 형태인데, Viagra에 대한 <strong>증거</strong>(evidence), 즉 <span class="math notranslate nohighlight">\(p\rm(Yes)\)</span>에 비해 스팸 메일에 Viagra 단어가 들어있는 <strong>가능도</strong>(likelihood), 즉 <span class="math notranslate nohighlight">\(p{\rm(Yes\mid spam)}\)</span>이 클수록 사후적 확률이 사전적 확률보다 더 커진다.</p>
<p>앞에서 예로 다루었던 <span class="math notranslate nohighlight">\(p\rm(spam\mid Yes)\)</span>를 베이즈 정리에 의해 계산해보자. 위 표 9.1을 보면 <span class="math notranslate nohighlight">\(p\rm (spam)=20/100\)</span>이고, <span class="math notranslate nohighlight">\(p\rm (Yes)=5/100\)</span>이다. 또한 <span class="math notranslate nohighlight">\(p{\rm(Yes\mid spam)}=4/20\)</span>이다. 따라서 식 9.4에 의해 계산하면 다음과 같이 0.8로서 앞에서 계산한 것과 동일하다.</p>
<div class="math notranslate nohighlight">
\[p{\rm(spam\mid Yes)}=\frac {p{\rm(spam)}\ p{\rm(Yes\mid spam)}}{p{\rm(Yes)}}=\frac{(20/100)\times(4/20)}{5/100}=\frac{4/100}{5/100}=0.8\]</div>
</section>
</section>
<section id="id6">
<h2>9.2 나이브 베이즈 분류 예제<a class="headerlink" href="#id6" title="Link to this heading">#</a></h2>
<p>앞의 스팸 메일 필터링 예제를 좀 더 발전시켜 보자. 이제 이메일 메시지에서 Viagra 외에 Money와 Groceries라는 단어를 추가적으로 검색할 수 있다. 이번에도 100개의 이메일이 있다. 아래 표에서 보듯이 각 이메일 별로 Viagra, Money, Groceries 단어가 들어가 있는지 여부가 Yes/No로 표시돼 있다. 이제 예측변수가 하나가 아니라 3개(<span class="math notranslate nohighlight">\(X_1,X_2,X_3\)</span>)이고, 이를 이용해 반응(<span class="math notranslate nohighlight">\(Y\)</span>)의 범주를 예측한다. 우리에게 주어진 테스트 관측은 아래 표의 맨 아래 0번 관측으로서 <span class="math notranslate nohighlight">\(\rm Viagra=Yes\)</span>, <span class="math notranslate nohighlight">\(\rm Money=Yes\)</span>, <span class="math notranslate nohighlight">\(\rm Groceries=No\)</span>이다.</p>
<p><img alt="나이브 베이즈 분류 예제" src="https://raw.githubusercontent.com/pilsunchoi/images/main/9-5.png" /></p>
<p>위 예제에서 테스트 이메일이 스팸일 <strong>사후적 확률</strong>은 <span class="math notranslate nohighlight">\(p({\rm spam}\mid x_{1},x_{2},x_{3})\)</span>으로 표현할 수 있으며, 이는 조건부확률의 정의에 의해 다음과 같다.(식 9.1 참조)</p>
<div class="math notranslate nohighlight">
\[p({\rm spam}\mid x_{1},x_{2},x_{3})={\frac {p({\rm spam}, x_{1},x_{2},x_{3})}{p(x_{1},x_{2},x_{3})}} \tag{9.5}\]</div>
<p>여기에서 우변의 분자는 다음과 같이 쓸 수 있는데, 첫 번째 등호의 경우, 결합확률은 괄호안 변수의 순서를 바꿔도 내용상 변화가 없다는 점을 이용한 것이고, 두 번째 등호부터는 조건부확률의 정의를 연쇄적으로 적용한 것이다.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
{\begin{aligned}p({\rm spam},x_{1},x_{2},x_{3})
&amp;=p(x_{1},x_{2},x_{3},{\rm spam})\\
&amp;=p(x_{1}\mid x_{2},x_{3},{\rm spam})\ p(x_{2},x_{3},{\rm spam})\\
&amp;=p(x_{1}\mid x_{2},x_{3},{\rm spam})\ p(x_{2}\mid x_{3},{\rm spam})\ p(x_{3},{\rm spam})\\
&amp;=p(x_{1}\mid x_{2},x_{3},{\rm spam})\ p(x_{2}\mid x_{3},{\rm spam})\ p(x_{3}\mid {\rm spam})\ p({\rm spam})
\end{aligned}}
\end{split}\]</div>
<p>우리는 이 단계에서 “순진한(naive)” <strong>조건부독립</strong>(conditionally independent) 가정을 적용한다. <span class="math notranslate nohighlight">\(A\)</span>, <span class="math notranslate nohighlight">\(B\)</span>, <span class="math notranslate nohighlight">\(C\)</span> 세 개의 사건(event)이 있을 때, 다음이 성립하면 <span class="math notranslate nohighlight">\(C\)</span>의 조건 하에서 <span class="math notranslate nohighlight">\(A\)</span>와 <span class="math notranslate nohighlight">\(B\)</span>는 조건부독립이다.(이는 <span class="math notranslate nohighlight">\(B\)</span>의 발생이 <span class="math notranslate nohighlight">\(A\)</span>의 확률에 영향을 미치지 않는 것을 의미한다.)</p>
<div class="math notranslate nohighlight">
\[p(A\mid B,C)=p(A\mid C)\]</div>
<p>이러한 조건부독립의 정의를 우리 예에 적용해보자. 만약 <span class="math notranslate nohighlight">\(X_{1},X_{2},X_{3}\)</span>가 <code class="docutils literal notranslate"><span class="pre">spam</span></code>의 조건 하에서 서로 조건부독립이면 다음이 성립한다.</p>
<div class="math notranslate nohighlight">
\[p(x_{1}\mid x_{2},x_{3},{\rm spam})=p(x_{1}\mid {\rm spam}),~~~~~p(x_{2}\mid x_{3},{\rm spam})=p(x_{2}\mid {\rm spam})\]</div>
<p>따라서 이상의 논의를 종합하면, 식 9.5의 조건부확률 <span class="math notranslate nohighlight">\(p({\rm spam}\mid x_{1},x_{2},x_{3})\)</span>은 조건부독립의 가정 하에서 다음과 같이 된다.</p>
<div class="math notranslate nohighlight">
\[
p({\rm spam}\mid x_{1},x_{2},x_{3})= p(x_{1}\mid {\rm spam})\ p(x_{2}\mid {\rm spam})\ p(x_{3}\mid {\rm spam})\ p({\rm spam})~/~Z \tag{9.6}
\]</div>
<p>여기서 <span class="math notranslate nohighlight">\(Z\)</span>는 <span class="math notranslate nohighlight">\(p(x_{1},x_{2},x_{3})\)</span>이다. 마찬가지로 <code class="docutils literal notranslate"><span class="pre">ham</span></code>에 대한 조건부확률은 다음과 같다.</p>
<div class="math notranslate nohighlight">
\[
p({\rm ham}\mid x_{1},x_{2},x_{3})= p(x_{1}\mid {\rm ham})\ p(x_{2}\mid {\rm ham})\ p(x_{3}\mid {\rm ham})\ p({\rm ham})~/~Z \tag{9.7}
\]</div>
<p>식 9.6과 9.7을 비교해보면 <span class="math notranslate nohighlight">\(Y\)</span> 클래스가 <code class="docutils literal notranslate"><span class="pre">spam</span></code>이든 <code class="docutils literal notranslate"><span class="pre">ham</span></code>이든 <span class="math notranslate nohighlight">\(Z=p(x_{1},x_{2},x_{3})\)</span>는 바뀌지 않는다. 따라서 양자를 비교할 때, <span class="math notranslate nohighlight">\(Z\)</span>는 계산하지 않고 나머지 부분만 비교하면 된다.</p>
<section id="id7">
<h3>스팸 메일 분류 예제<a class="headerlink" href="#id7" title="Link to this heading">#</a></h3>
<p>앞의 예에서 테스트 관측에 대해 스팸/햄의 확률을 구해보자. 0번 관측의 경우, <span class="math notranslate nohighlight">\(\rm Viagra=Yes\)</span>, <span class="math notranslate nohighlight">\(\rm Money=Yes\)</span>, <span class="math notranslate nohighlight">\(\rm Groceries=No\)</span>이다. 조건부확률 계산을 위해서는 가능도(likelihood)에 대한 정보가 필요한데, 아래 표와 같다고 하자. 표를 읽는 법은 가령 <span class="math notranslate nohighlight">\(p(X_{1}={\rm Yes}\mid Y={\rm spam})=4/20\)</span>이다.</p>
<p><strong>표 9.2.</strong> 스팸 메일 필터링 예제 3.</p>
<p><img alt="스팸 필터링 예제 3 표" src="https://raw.githubusercontent.com/pilsunchoi/images/main/9-6.png" /></p>
<ul class="simple">
<li><p>표 출처: Brett Lantz (2015), Machine Learing With R, 4장.</p></li>
</ul>
<p>위 표를 이용해 테스트 관측이 스팸일 확률을 구하면 다음과 같다.(식 9.6에서 <span class="math notranslate nohighlight">\(Z\)</span>를 제외한 부분만 계산함.)</p>
<div class="math notranslate nohighlight">
\[(4/20)\times(10/20)\times(20/20)\times(20/100)=0.02\]</div>
<p>다음으로 테스트 관측이 햄일 확률을 구하면 다음과 같다.(식 9.7에서 <span class="math notranslate nohighlight">\(Z\)</span>를 제외한 부분만 계산함.)</p>
<div class="math notranslate nohighlight">
\[(1/80)\times(16/80)\times(72/80)\times(80/100)=0.0018\]</div>
<p>두 값을 비교해보면, 0번 관측이 스팸일 확률이 햄일 확률에 비해 11배 이상 더 크다. 따라서 0번 관측은 스팸으로 분류된다.</p>
<p>사실 우리 예에서 Money 단어는 Viagra나 Groceries 단어와 상관성이 있을 가능성이 높다. 하지만 <strong>조건부독립 가정은</strong> (이들 단어 간에 상관성이 어찌됐든) <strong>이들 단어가 스팸일 확률에 독립적으로 기여한다고 가정</strong>하는 것이다.</p>
</section>
<section id="naive">
<h3>순진한(naive) 가정<a class="headerlink" href="#naive" title="Link to this heading">#</a></h3>
<p>조건부독립 가정이 실제 현실에 위반될 가능성이 많다는 점에서 우리는 이를 순진한(naive) 가정이라고 이름을 붙였다. 이렇게 순진한 가정을 하는 이유는 이것이 조건부확률 추정을 <strong>획기적으로 단순</strong>하게 만들어주기 때문이다. 왜냐하면 조건부확률을 추정함에 있어서 예측변수(즉 <span class="math notranslate nohighlight">\(x_1,x_2,x_3\)</span>) 상호간의 관계를 파악하지 않아도 되며, 오직 각 개별 예측변수와 주어진 클래스(스팸/햄) 간의 관계만 따지면 되기 때문이다.</p>
<p>사실 우리 예에서의 예측변수는 Yes/No만으로 이루어진 단순한 형태이지만 일반적으로는 연속형 변수까지 포함될 것이며, 거기에다 변수의 개수가 수십개 또는 수백개로 늘어나면 이들 간 확률적 관계를 파악하는 것이 사실상 불가능해질 수도 있다. 이런 상황에서 조건부독립 가정은 추정을 아주 간단하게 만들어준다.</p>
<p>나이브 베이즈에 의한 분류는  <strong>실행 속도</strong>가 상대적으로 빠르다. 조건부독립 가정 덕분에 예측변수에 대한 분석이 개별적 차원(즉 1차원)에서 독립적으로 이루어질 수 있기 때문이다. 소위 차원의 저주(curse of dimensionality) 문제를 해결해 준다.</p>
<p>또한 조건부독립 가정 덕분에 <strong>훈련 관측의 개수가 많지 않은 경우</strong>에도 조건부확률의 추정이 가능하다. 우리 예의 경우, 만약 조건부독립의 가정이 없다면, 100개의 이메일 중에서 Viagra와 Money 단어는 들어있고, Groceries 단어는 들어있지 않은 이메일을 일차적으로 파악해야 하는데, 그 조건에 부합하는 관측이 극히 소수(가령 1개)일 경우 그것만으로는 신뢰할 만한 조건부확률을 추정하기 어렵다.</p>
<p>지나치게 단순화된 가정에도 불구하고 나이브 베이즈 분류기는 <strong>실제 상황에서 상당히 잘 작동</strong>하는 것으로 평가받고 있다. 특히 문서 분류(document classification)나 스팸 필터링에서 좋은 성과를 나타냈다. Zhang(2004)은 겉보기에 단순한 베이즈 분류기가 놀라운 분류 성과를 보이는 데는 나름대로의 이론적 이유가 있음을 보여주었다. Caruana와 Niculescu-Mizil(2006)은 여러 분류 알고리즘을 종합적으로 비교한 결과, 베이즈 분류가 랜덤 포레스트(random forest)나 부스팅(boosting)과 같은 다른 접근 방식보다 성과가 우수함을 보여주었다.</p>
</section>
</section>
<section id="id8">
<h2>9.3 나이브 베이즈 알고리즘<a class="headerlink" href="#id8" title="Link to this heading">#</a></h2>
<p>앞에서 예제를 통해 살펴 본 나이브 베이즈 분류를 일반적 환경에서 이해해보자.</p>
<p>나이브 베이즈는 한 마디로 조건부확률 모델이다. <span class="math notranslate nohighlight">\(m\)</span>개의 예측변수 벡터 <span class="math notranslate nohighlight">\((X_{1},\ldots ,X_{m})\)</span>이 있고, 반응변수를 <span class="math notranslate nohighlight">\(Y\)</span>로 표기하자. 이 상황에서 나이브 베이즈는 주어진 예측변수 정보 <span class="math notranslate nohighlight">\((x_{1},\ldots ,x_{m})\)</span>을 이용해 다음의 조건부확률을 구하고자 한다.</p>
<div class="math notranslate nohighlight">
\[p(y\mid x_{1},\ldots ,x_{m})\]</div>
<p>조건부확률의 정의는 다음과 같다.</p>
<div class="math notranslate nohighlight">
\[p(y\mid x_{1},\ldots ,x_{m})={\frac {p(y,x_{1},\ldots ,x_{m})}{p(x_{1},\ldots ,x_{m})}}  \tag{9.8}\]</div>
<p>위 식 우변의 분자는 <span class="math notranslate nohighlight">\(p(x_{1},\ldots ,x_{m},y)\)</span>와 동일하며, 여기에 조건부확률 정의를 반복 적용하면 다음과 같이 다시 쓸 수 있다.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
{\begin{aligned}
p(y,x_{1},\ldots ,x_{m})
&amp;=p(x_{1},\ldots ,x_{m},y)\\
&amp;=p(x_{1}\mid x_{2},\ldots ,x_{m},y)\ p(x_{2},\ldots ,x_{m},y)\\
&amp;=p(x_{1}\mid x_{2},\ldots ,x_{m},y)\ p(x_{2}\mid x_{3},\ldots ,x_{m},y)\ p(x_{3},\ldots ,x_{m},y)\\
&amp;=\cdots \\
&amp;=p(x_{1}\mid x_{2},\ldots ,x_{m},y)\ p(x_{2}\mid x_{3},\ldots ,x_{m},y)\cdots p(x_{n-1}\mid x_{m},y)\ p(x_{m}\mid y)\ p(y)\end{aligned}}
\end{split}\]</div>
<p>여기에 순진한(naive) 조건부독립 가정을 적용하여, 모든 특성 <span class="math notranslate nohighlight">\((X_{1},\ldots ,x_{m})\)</span>이 클래스 <span class="math notranslate nohighlight">\(y\)</span> 내에서 조건부로 상호독립적(mutually independent)이라고 가정한다. 이 가정 하에서 다음이 성립한다.</p>
<div class="math notranslate nohighlight">
\[p(x_{i}\mid x_{i+1},\ldots ,x_{m},y)=p(x_{i}\mid y)\]</div>
<p>이를 적용하면 식 9.8의 조건부확률은 다음과 같이 된다.</p>
<div class="math notranslate nohighlight">
\[
p(y \mid x_{1}, \ldots, x_{m}) 
= \frac{p(x_{1} \mid y) \, p(x_{2} \mid y) \cdots p(x_{m} \mid y) \, p(y)}{p(x_{1}, \ldots, x_{m})} 
= \frac{1}{Z} p(y) \prod_{i=1}^{m} p(x_{i} \mid y)
\tag{9.9}
\]</div>
<p>여기서 <span class="math notranslate nohighlight">\(Z=p(x_{1},\ldots ,x_{m})\)</span>은 모든 클래스 <span class="math notranslate nohighlight">\(y\)</span>에 동일하게 적용되는 배율 인수(scaling factor)이다.</p>
<section id="id9">
<h3>확률 모델을 이용한 분류기<a class="headerlink" href="#id9" title="Link to this heading">#</a></h3>
<p>이로써 식 9.9의 나이브 베이즈 확률 모델을 도출했다. 이를 분류 작업에 적용하려면 결정규칙과 결합시켜야 한다. 가장 일반적이고 직관적인 규칙은 앞에서도 언급했듯이 가장 확률이 높은 클래스를 선택하는 것이다. 이것을 MAP(maximum a posteriori) 결정규칙이라고 부른다. 나이브 베이즈 확률 모델인 식 9.9에 MAP 결정규칙을 적용할 경우, 테스트 관측 <span class="math notranslate nohighlight">\((x_{1},\ldots ,x_{m})\)</span>의 클래스로서 다음을 만족하는 <span class="math notranslate nohighlight">\(\hat y\)</span>을 선택한다는 것을 의미한다.(<span class="math notranslate nohighlight">\(Z\)</span>는 상수이기 때문에 식에서 빼도 된다.)</p>
<div class="math notranslate nohighlight">
\[{\hat {y}}={\underset {y}{\operatorname {argmax} }}\ p(y)\displaystyle \prod _{i=1}^{m}p(x_{i}\mid y) \tag{9.10}\]</div>
</section>
<section id="id10">
<h3>사전적 확률 추정<a class="headerlink" href="#id10" title="Link to this heading">#</a></h3>
<p><span class="math notranslate nohighlight">\(p(y)\)</span>는 훈련 데이터세트에서 클래스 <span class="math notranslate nohighlight">\(y\)</span>의 빈도를 카운트함으로써 쉽게 추정이 가능하다. 앞의 표 9.1 및 9.2의 스팸 메일 필터링 예에서 100개 중 20개가 <code class="docutils literal notranslate"><span class="pre">spam</span></code>이고 나머지 80개가 <code class="docutils literal notranslate"><span class="pre">ham</span></code>이기 때문에 <span class="math notranslate nohighlight">\(p(\rm spam)=0.2\)</span>이고, <span class="math notranslate nohighlight">\(p(\rm ham)=0.8\)</span>이다.</p>
</section>
<section id="id11">
<h3>가능도 추정<a class="headerlink" href="#id11" title="Link to this heading">#</a></h3>
<p><span class="math notranslate nohighlight">\(p(x_{i}\mid y)\)</span>의 추정은 예측변수(즉 <span class="math notranslate nohighlight">\(X_i\)</span>)의 종류에 따라 다른 접근법이 사용된다.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(X_i\)</span>가 <strong>정량적</strong>(quantitative) 변수이면 일반적으로 <span class="math notranslate nohighlight">\(X_i|y ∼ N(\mu_{yi},\sigma^2_{yi})\)</span>라고 가정한다. 즉, 클래스 <span class="math notranslate nohighlight">\(y\)</span> 내에서 예측변수 <span class="math notranslate nohighlight">\(X_i\)</span>는 <strong>정규 분포</strong>(normal/Gaussian distribution)를 따른다고 가정한다. 이 가정 하에서 <span class="math notranslate nohighlight">\(p(x_{i}\mid y)\)</span>을 추정하기 위해서는 먼저 해당 클래스 <span class="math notranslate nohighlight">\(y\)</span> 내에서 <span class="math notranslate nohighlight">\(x_i\)</span>의 평균 <span class="math notranslate nohighlight">\(\mu_{yi}\)</span>와 분산 <span class="math notranslate nohighlight">\(\sigma^2_{yi}\)</span>를 추정한다. 이것들은 단순히 표본평균과 표본분산을 계산하면 된다. 그런 다음, 그것을 다음과 같이 정규 분포 확률밀도 함수에 대입하여 <span class="math notranslate nohighlight">\(p(x_{i}\mid y)\)</span>를 계산한다. 이런 방식으로 <span class="math notranslate nohighlight">\(p(x_{i}\mid y)\)</span>를 추정하는 것을 <strong>가우시안 나이브 베이즈</strong>(Gaussian Naive Bayes)라고 부른다.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[p(x_{i}\mid y)={\frac {1}{\sqrt {2\pi \sigma _{yi}^{2}}}}\,\exp \left({-{\frac {(x_i-\mu_{yi})^{2}}{2\sigma^2_{yi}}}}\right)\]</div>
</br>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(X_i\)</span>가 <strong>정성적</strong>(qualitative)인 경우는 이미 앞에서 다루었다. 가령 앞의 표 9.2 예에서 Viagra 단어를 의미하는 <span class="math notranslate nohighlight">\(X_1\)</span>은 <span class="math notranslate nohighlight">\(\{\rm Yes, No\}\)</span>의 <strong>이항</strong>(binary) 범주로 돼있다. 이 경우, <span class="math notranslate nohighlight">\(p(X_1={\rm Yes}\mid Y=\rm spam)\)</span>을 구하기 위해서는 전체 스팸 메일 20개 중 Viagra 단어를 포함한 이메일의 비율을 구하면 되고, 가령 <span class="math notranslate nohighlight">\(p(X_1={\rm No}\mid Y=\rm ham)\)</span>이면 전체 햄 메일 80개 중 Viagra 단어가 들어있지 않은 이메일의 비율을 구하면 된다. 이런 방식으로 <span class="math notranslate nohighlight">\(p(x_{i}\mid y)\)</span>를 추정하는 것을 <strong>베르누이 나이브 베이즈</strong>(Bernoulli Naive Bayes)라고 부른다.
</br></p></li>
<li><p><span class="math notranslate nohighlight">\(X_i\)</span>가 정성적 변수로서 범주가 2개보다 많은 <strong>다항</strong>(multinomial)인 경우에도 동일한 방식으로 추정할 수 있다. 예를 들어, <span class="math notranslate nohighlight">\(X_i\)</span>의 범주가 <span class="math notranslate nohighlight">\(\{1, 2, 3\}\)</span>으로 구성돼 있다고 하자. 그리고 주어진 <span class="math notranslate nohighlight">\(y\)</span> 클래스에 100개의 관측이 있다고 하자. 이 상황에서 예측변수 <span class="math notranslate nohighlight">\(X_i\)</span>의 100개 관측 중 가령 32, 55, 13개의 관측이 각각 1, 2, 3의 값을 취한다고 하자. 그러면 <span class="math notranslate nohighlight">\(p(x_{i}\mid y)\)</span>를 다음과 같이 추정할 수 있으며, 이를 <strong>다항 나이브 베이즈</strong>(Multinomial Naive Bayes)라고 부른다.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
p(x_{i}\mid y)=
\begin{cases}
0.32, &amp; \text{if }x_i=1 \\
0.55, &amp; \text{if }x_i=2 \\
0.13, &amp; \text{if }x_i=3 \\
\end{cases}
\end{split}\]</div>
</section>
</section>
<section id="id12">
<h2>9.4 나이브 베이즈 실습<a class="headerlink" href="#id12" title="Link to this heading">#</a></h2>
<section id="id13">
<h3>주식시장 데이터<a class="headerlink" href="#id13" title="Link to this heading">#</a></h3>
<p>ISLP에서 제공하는 <code class="docutils literal notranslate"><span class="pre">Smarket</span></code> 데이터세트는 2001년 초부터 2005년 말까지 1,250일 동안 S&amp;P 500 주가지수의 수익률(%)을 기록한 것이다.</p>
<p><code class="docutils literal notranslate"><span class="pre">Today</span></code>는 각 날짜의 수익률이고, 각 날짜에 대해 이전 5개 거래일의 수익률을 <code class="docutils literal notranslate"><span class="pre">Lag1</span></code>부터 <code class="docutils literal notranslate"><span class="pre">Lag5</span></code>까지의 이름으로 기록했다. 또한 <code class="docutils literal notranslate"><span class="pre">Volume</span></code>은 전날 거래된 주식 수(단위: 십억)이고, <code class="docutils literal notranslate"><span class="pre">Direction</span></code>은 해당 날짜에 시장이 상승(<code class="docutils literal notranslate"><span class="pre">Up</span></code>) 또는 하락(<code class="docutils literal notranslate"><span class="pre">Down</span></code>)했는지를 나타낸다. <code class="docutils literal notranslate"><span class="pre">Year</span></code>는 각 날짜가 속한 연도이다.</p>
<p>앞의 로지스틱 회귀 및 KNN에 의한 분류에서 이 데이터세트를 분석했기 때문에 아래 나이브 베이즈 분류의 성과를 그것들과 비교해보자.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span> 

<span class="n">Smarket</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;../Data/Smarket.csv&#39;</span><span class="p">)</span>
<span class="n">Smarket</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Year</th>
      <th>Lag1</th>
      <th>Lag2</th>
      <th>Lag3</th>
      <th>Lag4</th>
      <th>Lag5</th>
      <th>Volume</th>
      <th>Today</th>
      <th>Direction</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2001</td>
      <td>0.381</td>
      <td>-0.192</td>
      <td>-2.624</td>
      <td>-1.055</td>
      <td>5.010</td>
      <td>1.1913</td>
      <td>0.959</td>
      <td>Up</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2001</td>
      <td>0.959</td>
      <td>0.381</td>
      <td>-0.192</td>
      <td>-2.624</td>
      <td>-1.055</td>
      <td>1.2965</td>
      <td>1.032</td>
      <td>Up</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2001</td>
      <td>1.032</td>
      <td>0.959</td>
      <td>0.381</td>
      <td>-0.192</td>
      <td>-2.624</td>
      <td>1.4112</td>
      <td>-0.623</td>
      <td>Down</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2001</td>
      <td>-0.623</td>
      <td>1.032</td>
      <td>0.959</td>
      <td>0.381</td>
      <td>-0.192</td>
      <td>1.2760</td>
      <td>0.614</td>
      <td>Up</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2001</td>
      <td>0.614</td>
      <td>-0.623</td>
      <td>1.032</td>
      <td>0.959</td>
      <td>0.381</td>
      <td>1.2057</td>
      <td>0.213</td>
      <td>Up</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Smarket</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(1250, 9)
</pre></div>
</div>
</div>
</div>
</section>
<section id="id14">
<h3>나이브 베이즈 분류<a class="headerlink" href="#id14" title="Link to this heading">#</a></h3>
<p>나이브 베이즈 분류의 예측 정확도를 테스트 데이터세트를 이용하여 평가하기 위해 2001년부터 2004년까지의 관측을 훈련 세트로 하고, 2005년의 관측을 테스트 세트로 한다. 또한 예측변수로는 앞의 KNN 분류에서와 마찬가지로 <code class="docutils literal notranslate"><span class="pre">Lag1</span></code>, <code class="docutils literal notranslate"><span class="pre">Lag2</span></code> 변수만 사용한다.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_df</span> <span class="o">=</span> <span class="n">Smarket</span><span class="p">[</span><span class="n">Smarket</span><span class="o">.</span><span class="n">Year</span> <span class="o">&lt;</span> <span class="mi">2005</span><span class="p">]</span>
<span class="n">test_df</span> <span class="o">=</span> <span class="n">Smarket</span><span class="p">[</span><span class="n">Smarket</span><span class="o">.</span><span class="n">Year</span> <span class="o">==</span> <span class="mi">2005</span><span class="p">]</span>

<span class="n">X_train</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">[[</span><span class="s1">&#39;Lag1&#39;</span><span class="p">,</span><span class="s1">&#39;Lag2&#39;</span><span class="p">]]</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">[</span><span class="s1">&#39;Direction&#39;</span><span class="p">]</span>

<span class="n">X_test</span> <span class="o">=</span> <span class="n">test_df</span><span class="p">[[</span><span class="s1">&#39;Lag1&#39;</span><span class="p">,</span><span class="s1">&#39;Lag2&#39;</span><span class="p">]]</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">test_df</span><span class="p">[</span><span class="s1">&#39;Direction&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p><strong><code class="docutils literal notranslate"><span class="pre">GaussianNB</span></code> 함수</strong></p>
<p>예측변수 <code class="docutils literal notranslate"><span class="pre">Lag1</span></code>과 <code class="docutils literal notranslate"><span class="pre">Lag2</span></code>가 모두 수익률로서 정량적 변수이기 때문에 <strong>가우시안 나이브 베이즈</strong>가 적절하다. 이를 수행하기 위해 사이킷런(<code class="docutils literal notranslate"><span class="pre">sklearn</span></code>)의 <code class="docutils literal notranslate"><span class="pre">naive_bayes</span></code> 모듈에서 <code class="docutils literal notranslate"><span class="pre">GaussianNB</span></code> 함수를 불러들인다.</p>
<p>모델을 가우시안 나이브 베이즈(<code class="docutils literal notranslate"><span class="pre">GaussianNB</span></code>)로 선택하고, 모델의 이름을 <code class="docutils literal notranslate"><span class="pre">gnb</span></code>로 지정했다. 그런 다음, <code class="docutils literal notranslate"><span class="pre">fit</span></code> 메서드를 사용하여 모델을 피팅하는데, 이를 위해 예측변수(<code class="docutils literal notranslate"><span class="pre">X_train</span></code>)와 반응변수(<code class="docutils literal notranslate"><span class="pre">y_train</span></code>)를 입력한다. 그 피팅 결과를 바탕으로 <code class="docutils literal notranslate"><span class="pre">predict</span></code> 메서드를 이용해 테스트 세트 관측(<code class="docutils literal notranslate"><span class="pre">X_test</span></code>)에 대해 클래스를 예측하도록 하여 <code class="docutils literal notranslate"><span class="pre">pred</span></code>라는 이름으로 지정했다.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span>

<span class="n">gnb</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span>
<span class="n">pred</span> <span class="o">=</span> <span class="n">gnb</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">confusion_matrix()</span></code> 함수를 사용해서 <code class="docutils literal notranslate"><span class="pre">pred</span></code>로 지정해 놓은 예측 클래스를 테스트 세트 실제 클래스(<code class="docutils literal notranslate"><span class="pre">y_test</span></code>)와 비교하여 혼동행렬(confusion matrix)을 만들었다.(혼동행렬에 대해서는 7장 부록 참조.) 또한 <code class="docutils literal notranslate"><span class="pre">classification_report()</span></code> 함수를 사용해서 정밀도(precision), 재현율(recall) 등을 출력했다.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">classification_report</span><span class="p">,</span> <span class="n">confusion_matrix</span>

<span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">pred</span><span class="p">)</span> 
<span class="n">cm_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">cm</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Down&#39;</span><span class="p">,</span><span class="s1">&#39;Up&#39;</span><span class="p">],</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Down&#39;</span><span class="p">,</span><span class="s1">&#39;Up&#39;</span><span class="p">])</span>
<span class="n">cm_df</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;True&#39;</span>
<span class="n">cm_df</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;Predicted&#39;</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cm_df</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">digits</span><span class="o">=</span><span class="mi">3</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Predicted  Down   Up
True                
Down         29   82
Up           20  121

              precision    recall  f1-score   support

        Down      0.592     0.261     0.362       111
          Up      0.596     0.858     0.703       141

    accuracy                          0.595       252
   macro avg      0.594     0.560     0.533       252
weighted avg      0.594     0.595     0.553       252
</pre></div>
</div>
</div>
</div>
<p>위 결과를 보면, 정확도(accuracy)가 59.5%이다. 테스트 세트의 252 거래일 중 150 거래일의 상승과 하락을 제대로 맞췄다. 아래에서 보듯이 테스트 세트에서 56.0%가 상승이고, 나머지 44.0%가 하락이다. 따라서 테스트 세트의 모든 관측을 <code class="docutils literal notranslate"><span class="pre">Up</span></code>으로 예측할 경우, 정확도는 56.0%가 된다. 이것보다는 정확도가 높다는 점에서 어느 정도 예측력을 인정해 줄 수 있지만, 그 차이가 그다지 크지는 않다.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test_df</span><span class="o">.</span><span class="n">Direction</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">test_df</span><span class="o">.</span><span class="n">Direction</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Direction
Up      0.559524
Down    0.440476
Name: count, dtype: float64
</pre></div>
</div>
</div>
</div>
</section>
<section id="id15">
<h3>모델별 정확도 비교<a class="headerlink" href="#id15" title="Link to this heading">#</a></h3>
<p>우리는 앞 장에서도 동일한 주식시장 데이터세트를 사용해서 주가의 상승/하락을 예측해보았다. 동일한 예측변수(즉 <code class="docutils literal notranslate"><span class="pre">Lag1</span></code>, <code class="docutils literal notranslate"><span class="pre">Lag2</span></code>)와 동일한 훈련/테스트 세트를 사용하여 지금까지 세 가지 모델로 상승/하락을 예측했는데, 이들의 정확도(accuracy)와 정밀도(Precision: Up)를 비교한 표가 아래 나와 있다. KNN(K-nearest neighbors)의 경우 <span class="math notranslate nohighlight">\(K=3\)</span>인 경우가 예측 성과가 가장 좋았기 때문에 그것을 사용하였다.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{array}{l|ccc}
           \text{Model} &amp; \text{Accuracy}  &amp; \text{Precision (Up)} \\ \hline
           \text{Logistic} &amp; 0.560 &amp; 0.582 \\
           \text{KNN }(K=3) &amp; 0.532 &amp; 0.577 \\
           \text{Naive Bayes} &amp; 0.595 &amp; 0.596 \\
\end{array}
\end{split}\]</div>
<p>위 결과를 보면, 나이브 베이즈가 정확도와 정밀도 모두에서 예측 성과가 가장 좋다. 여기에서 정밀도는 <code class="docutils literal notranslate"><span class="pre">Up</span></code>에 대한 것으로서 시장이 상승할 것으로 예측했는데, 실제로 상승한 비율을 의미한다. <code class="docutils literal notranslate"><span class="pre">Up</span></code>에 대한 정밀도가 높다면, 모델이 상승을 예측할 때는 매수하고 하락을 예측할 때는 거래를 피하는 전략을 생각해볼 수 있다.</p>
</section>
<section id="id16">
<h3>타이타닉 데이터<a class="headerlink" href="#id16" title="Link to this heading">#</a></h3>
<p>타이타닉호는 1912년 미국 뉴욕으로 향하던 첫 항해 중에 빙산과 충돌하여 침몰하여 2,224명의 승객 및 승무원 중 1,502명이 사망했다. <code class="docutils literal notranslate"><span class="pre">Titanic</span></code> 데이터세트는 Kaggle에서 주최한 머신러닝 대회 “<a class="reference external" href="https://www.kaggle.com/competitions/titanic/overview">Titanic - Machine Learning from Disaster</a>”에서 제공한 것을 사용한다.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Titanic</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;http://bit.ly/kaggletrain&#39;</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="s1">&#39;PassengerId&#39;</span><span class="p">)</span>
<span class="n">Titanic</span><span class="o">.</span><span class="n">tail</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Name</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Ticket</th>
      <th>Fare</th>
      <th>Cabin</th>
      <th>Embarked</th>
    </tr>
    <tr>
      <th>PassengerId</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>887</th>
      <td>0</td>
      <td>2</td>
      <td>Montvila, Rev. Juozas</td>
      <td>male</td>
      <td>27.0</td>
      <td>0</td>
      <td>0</td>
      <td>211536</td>
      <td>13.00</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>888</th>
      <td>1</td>
      <td>1</td>
      <td>Graham, Miss. Margaret Edith</td>
      <td>female</td>
      <td>19.0</td>
      <td>0</td>
      <td>0</td>
      <td>112053</td>
      <td>30.00</td>
      <td>B42</td>
      <td>S</td>
    </tr>
    <tr>
      <th>889</th>
      <td>0</td>
      <td>3</td>
      <td>Johnston, Miss. Catherine Helen "Carrie"</td>
      <td>female</td>
      <td>NaN</td>
      <td>1</td>
      <td>2</td>
      <td>W./C. 6607</td>
      <td>23.45</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>890</th>
      <td>1</td>
      <td>1</td>
      <td>Behr, Mr. Karl Howell</td>
      <td>male</td>
      <td>26.0</td>
      <td>0</td>
      <td>0</td>
      <td>111369</td>
      <td>30.00</td>
      <td>C148</td>
      <td>C</td>
    </tr>
    <tr>
      <th>891</th>
      <td>0</td>
      <td>3</td>
      <td>Dooley, Mr. Patrick</td>
      <td>male</td>
      <td>32.0</td>
      <td>0</td>
      <td>0</td>
      <td>370376</td>
      <td>7.75</td>
      <td>NaN</td>
      <td>Q</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p><strong>변수</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Survived</span></code>: 생존 더미변수(0 = No, 1 = Yes)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Pclass</span></code>: 티켓 등급(1 = 1st, 2 = 2nd, 3 = 3rd)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Sex</span></code>: 성별</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Age</span></code>: 나이(년)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">SibSp</span></code>: 타이타닉에 함께 승선한 형제/배우자 숫자</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Parch</span></code>: 타이타닉에 함께 승선한 부모/자녀 숫자</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Ticket</span></code>: 티켓 번호</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Fare</span></code>: 여객 운임</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Cabin</span></code>: 캐빈 번호</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Embarked</span></code>: 승선 항구(C = Cherbourg, Q = Queenstown, S = Southampton)</p></li>
</ul>
<p>아래 데이터세트 정보를 보면 <code class="docutils literal notranslate"><span class="pre">Age</span></code>, <code class="docutils literal notranslate"><span class="pre">Cabin</span></code>, <code class="docutils literal notranslate"><span class="pre">Embarked</span></code> 변수에 결측값이 있는 것을 알 수 있다.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Titanic</span><span class="o">.</span><span class="n">info</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
Index: 891 entries, 1 to 891
Data columns (total 11 columns):
 #   Column    Non-Null Count  Dtype  
---  ------    --------------  -----  
 0   Survived  891 non-null    int64  
 1   Pclass    891 non-null    int64  
 2   Name      891 non-null    object 
 3   Sex       891 non-null    object 
 4   Age       714 non-null    float64
 5   SibSp     891 non-null    int64  
 6   Parch     891 non-null    int64  
 7   Ticket    891 non-null    object 
 8   Fare      891 non-null    float64
 9   Cabin     204 non-null    object 
 10  Embarked  889 non-null    object 
dtypes: float64(2), int64(4), object(5)
memory usage: 83.5+ KB
</pre></div>
</div>
</div>
</div>
</section>
<section id="id17">
<h3>성별에 의한 생존 여부 예측<a class="headerlink" href="#id17" title="Link to this heading">#</a></h3>
<p>우선 아무런 사전 정보가 없는 상태에서 승객의 생존 여부에 대한 사전적 확률을 예측해보자. 이를 위해서는 <code class="docutils literal notranslate"><span class="pre">Survived</span></code> 변수의 클래스별 상대적 빈도를 구하면 된다. 아래 결과에서 보듯이 <span class="math notranslate nohighlight">\(p(\rm Survived=0)=0.616\)</span>이고, <span class="math notranslate nohighlight">\(p(\rm Survived=1)=0.384\)</span>이다. 확률론적 접근에 따르면, 사망 확률이 생존 확률보다 더 크기 때문에 어떤 승객이 주어졌을 때 사망으로 예측하는 것이 오분류를 줄이는 방법이다.</p>
<p>이에 따라 훈련 데이터의 승객 891명에 대해 모두 <span class="math notranslate nohighlight">\(\rm Survived=1\)</span>로 분류할 경우, 정확도는 61.6%(오분류율 38.4%)가 될 것이다.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Titanic</span><span class="o">.</span><span class="n">Survived</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">Titanic</span><span class="o">.</span><span class="n">Survived</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Survived
0    0.616162
1    0.383838
Name: count, dtype: float64
</pre></div>
</div>
</div>
</div>
<p><strong>성별 정보를 이용한 생존 여부 예측</strong></p>
<p>이제 예시의 목적으로 성별 변수(<code class="docutils literal notranslate"><span class="pre">Sex</span></code>)만을 사용하여 생존 여부를 예측해보자. 이를 위해 먼저 <code class="docutils literal notranslate"><span class="pre">female</span></code>/<code class="docutils literal notranslate"><span class="pre">male</span></code>의 문자형 변수인 <code class="docutils literal notranslate"><span class="pre">Sex</span></code> 변수를 <code class="docutils literal notranslate"><span class="pre">Female</span></code> 더미변수(<code class="docutils literal notranslate"><span class="pre">female</span></code>이면 1, <code class="docutils literal notranslate"><span class="pre">male</span></code>이면 0)로 전환시킨다.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Titanic</span><span class="p">[</span><span class="s1">&#39;Female&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="k">if</span> <span class="n">d</span> <span class="o">==</span> <span class="s1">&#39;female&#39;</span> <span class="k">else</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">Titanic</span><span class="o">.</span><span class="n">Sex</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p><strong>분할표 만들기</strong></p>
<p><code class="docutils literal notranslate"><span class="pre">pandas</span></code>에서 제공하는 <code class="docutils literal notranslate"><span class="pre">crosstab()</span></code> 함수를 사용해서 두 범주형 변수인 <code class="docutils literal notranslate"><span class="pre">Survived</span></code>와 <code class="docutils literal notranslate"><span class="pre">Female</span></code> 간의 교차빈도표, 즉 분할표를 만들어보자.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">crosstable</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">crosstab</span><span class="p">(</span><span class="n">index</span> <span class="o">=</span> <span class="n">Titanic</span><span class="p">[</span><span class="s1">&#39;Survived&#39;</span><span class="p">],</span>
                   <span class="n">columns</span> <span class="o">=</span> <span class="n">Titanic</span><span class="p">[</span><span class="s1">&#39;Female&#39;</span><span class="p">],</span>
                   <span class="n">margins</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>   <span class="c1"># 행과 열의 합을 표시하기</span>
<span class="n">crosstable</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>Female</th>
      <th>0</th>
      <th>1</th>
      <th>All</th>
    </tr>
    <tr>
      <th>Survived</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>468</td>
      <td>81</td>
      <td>549</td>
    </tr>
    <tr>
      <th>1</th>
      <td>109</td>
      <td>233</td>
      <td>342</td>
    </tr>
    <tr>
      <th>All</th>
      <td>577</td>
      <td>314</td>
      <td>891</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>위 분할표에 의해 여성과 남성 각각에 대해 생존 확률, 즉 조건부확률을 구할 수 있다. 다시 말하면, 생존에 대한 사후적 확률을 구할 수 있다.</p>
<div class="math notranslate nohighlight">
\[p(\rm Survived=1 \mid Female=1)=233/314=0.742\]</div>
<div class="math notranslate nohighlight">
\[p(\rm Survived=1 \mid Female=0)=109/577=0.189\]</div>
<p>여성(<span class="math notranslate nohighlight">\(\rm Female=1\)</span>)의 경우에는 생존 확률(74.2%)이 사망 확률(25.8%)보다 높고, 반대로 남성(<span class="math notranslate nohighlight">\(\rm Female=0\)</span>)은 생존 확률(18.9%)이 사망 확률(81.1%)보다 낮다. 따라서 어떤 승객이 주어졌을 때 여성이면 생존으로 예측하고, 남성이면 사망으로 예측하는 것이 최선이다.</p>
<p>이와 같은 식으로 훈련 데이터 승객 891명의 생존 여부를 분류할 경우 정확도를 계산해보자. 위 분할표에서 보듯이 전체 승객 891명 중 여성으로서 생존자는 233명이고, 남성으로서 사망자는 468명이기 때문에 여성을 생존, 남성을 사망으로 예측할 경우 정확도는 78.7%(오분류율 21.3%)가 된다.</p>
<div class="math notranslate nohighlight">
\[ \rm Accuracy = \frac{233+468}{891}=0.787\]</div>
<p><strong><code class="docutils literal notranslate"><span class="pre">BernoulliNB</span></code> 함수를 이용한 분석</strong></p>
<p>위 예제는 예측변수가 <code class="docutils literal notranslate"><span class="pre">Female</span></code> 하나뿐이어서 분할표 작성만으로 조건부확률을 구할 수 있지만, 여기에서는 예시의 목적으로 동일한 작업을 나이브 베이즈 알고리즘으로 실행시켜보자. 예측변수인 <code class="docutils literal notranslate"><span class="pre">Female</span></code>이 이항(bianry)의 정성적 변수이기 때문에 <strong>베르누이 나이브 베이즈</strong>가 적절하다. 이를 수행하기 위해 사이킷런(<code class="docutils literal notranslate"><span class="pre">sklearn</span></code>)의 <code class="docutils literal notranslate"><span class="pre">naive_bayes</span></code> 모듈에서 <code class="docutils literal notranslate"><span class="pre">BernoulliNB</span></code> 함수를 불러 들인다.</p>
<p>이제 모델을 베르누이 나이브 베이즈(<code class="docutils literal notranslate"><span class="pre">BernoulliNB</span></code>)로 선택하고, 모델의 이름을 <code class="docutils literal notranslate"><span class="pre">clf</span></code>로 지정했다. 그런 다음, <code class="docutils literal notranslate"><span class="pre">fit</span></code> 메서드를 사용하여 모델을 피팅하는데, 예측변수는 <code class="docutils literal notranslate"><span class="pre">Titanic[['Female']]</span></code>이고 반응변수는 <code class="docutils literal notranslate"><span class="pre">Titanic.Survived</span></code>이다. 그 피팅 결과를 바탕으로 <code class="docutils literal notranslate"><span class="pre">predict</span></code> 메서드를 이용해 훈련 세트(<code class="docutils literal notranslate"><span class="pre">Titanic[['Female']]</span></code>)에 대해 <code class="docutils literal notranslate"><span class="pre">Survived</span></code> 범주를 예측하도록 하여 <code class="docutils literal notranslate"><span class="pre">pred</span></code>라는 이름으로 지정했다.</p>
<p>아래 결과를 보면, 혼동행렬 자체가 앞의 분할표와 정확히 일치하는 것을 알 수 있다.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">BernoulliNB</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">BernoulliNB</span><span class="p">()</span>
<span class="n">clf_fit</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Titanic</span><span class="p">[[</span><span class="s1">&#39;Female&#39;</span><span class="p">]],</span> <span class="n">Titanic</span><span class="o">.</span><span class="n">Survived</span><span class="p">)</span>
<span class="n">pred</span> <span class="o">=</span> <span class="n">clf_fit</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Titanic</span><span class="p">[[</span><span class="s1">&#39;Female&#39;</span><span class="p">]])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">Titanic</span><span class="o">.</span><span class="n">Survived</span><span class="p">,</span> <span class="n">pred</span><span class="p">)</span> 
<span class="n">cm_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">cm</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;0&#39;</span><span class="p">,</span><span class="s1">&#39;1&#39;</span><span class="p">],</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;0&#39;</span><span class="p">,</span><span class="s1">&#39;1&#39;</span><span class="p">])</span>
<span class="n">cm_df</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;True&#39;</span>
<span class="n">cm_df</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;Predicted&#39;</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cm_df</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">Titanic</span><span class="o">.</span><span class="n">Survived</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">digits</span><span class="o">=</span><span class="mi">3</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Predicted    0    1
True               
0          468   81
1          109  233

              precision    recall  f1-score   support

           0      0.811     0.852     0.831       549
           1      0.742     0.681     0.710       342

    accuracy                          0.787       891
   macro avg      0.777     0.767     0.771       891
weighted avg      0.785     0.787     0.785       891
</pre></div>
</div>
</div>
</div>
<p><strong>로지스틱 회귀 모델 추정</strong></p>
<p>예시의 목적으로 이번에는 앞에서 배운 로지스틱 회귀 모델로 생존 확률을 예측하고, 그것에 기반하여 생존 여부를 분류해보자. <code class="docutils literal notranslate"><span class="pre">statsmodels.formula.api</span></code> 모듈의 <code class="docutils literal notranslate"><span class="pre">logit()</span></code> 함수를 이용하면 된다. 반응변수는 생존 더미변수인 <code class="docutils literal notranslate"><span class="pre">Survived</span></code>이고, 예측변수는 <code class="docutils literal notranslate"><span class="pre">Female</span></code>이다.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">statsmodels.formula.api</span> <span class="k">as</span> <span class="nn">smf</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">logit</span><span class="p">(</span><span class="s1">&#39;Survived ~ Female&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">Titanic</span><span class="p">)</span>
<span class="n">logitfit</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">logitfit</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Optimization terminated successfully.
         Current function value: 0.515041
         Iterations 5
                           Logit Regression Results                           
==============================================================================
Dep. Variable:               Survived   No. Observations:                  891
Model:                          Logit   Df Residuals:                      889
Method:                           MLE   Df Model:                            1
Date:                Fri, 17 Oct 2025   Pseudo R-squ.:                  0.2266
Time:                        23:58:00   Log-Likelihood:                -458.90
converged:                       True   LL-Null:                       -593.33
Covariance Type:            nonrobust   LLR p-value:                 2.020e-60
==============================================================================
                 coef    std err          z      P&gt;|z|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept     -1.4571      0.106    -13.701      0.000      -1.666      -1.249
Female         2.5137      0.167     15.036      0.000       2.186       2.841
==============================================================================
</pre></div>
</div>
</div>
</div>
<p>아래는 생존 확률 분계점으로 0.5를 사용하여 훈련 세트에 대해 생존 여부를 분류한 것이다. 혼동행렬 자체가 앞의 베르누이 나이브 베이즈와 정확히 일치하는 것을 확인할 수 있다.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">logit_pred</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">logitfit</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Titanic</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">)</span><span class="o">.</span><span class="n">map</span><span class="p">({</span><span class="kc">False</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">True</span><span class="p">:</span> <span class="mi">1</span><span class="p">})</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">Titanic</span><span class="o">.</span><span class="n">Survived</span><span class="p">,</span> <span class="n">logit_pred</span><span class="p">)</span>  
<span class="n">cm_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">cm</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">cm_df</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;True&#39;</span>
<span class="n">cm_df</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;Predicted&#39;</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cm_df</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">Titanic</span><span class="o">.</span><span class="n">Survived</span><span class="p">,</span> <span class="n">logit_pred</span><span class="p">,</span> <span class="n">digits</span><span class="o">=</span><span class="mi">3</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Predicted    0    1
True               
0          468   81
1          109  233

              precision    recall  f1-score   support

           0      0.811     0.852     0.831       549
           1      0.742     0.681     0.710       342

    accuracy                          0.787       891
   macro avg      0.777     0.767     0.771       891
weighted avg      0.785     0.787     0.785       891
</pre></div>
</div>
</div>
</div>
</section>
<section id="id18">
<h3>나이 및 운임에 의한 생존 여부 예측<a class="headerlink" href="#id18" title="Link to this heading">#</a></h3>
<p>이번에도 예시의 목적으로 나이(<code class="docutils literal notranslate"><span class="pre">Age</span></code>)와 여객 운임(<code class="docutils literal notranslate"><span class="pre">Fare</span></code>) 변수를 사용하여 생존 여부를 예측해보자. 두 예측변수 모두 정량적 변수들이다.</p>
<p>먼저 <code class="docutils literal notranslate"><span class="pre">Age</span></code> 변수에 결측값이 있는데, 이를 제외시키면 관측이 714개로 줄어든다. 그런 다음, <code class="docutils literal notranslate"><span class="pre">Titanic</span></code> 데이터세트에서 <code class="docutils literal notranslate"><span class="pre">Age</span></code>와 <code class="docutils literal notranslate"><span class="pre">Fare</span></code> 변수만을 골라 <code class="docutils literal notranslate"><span class="pre">X</span></code>라는 이름으로 지정한다.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Titanic</span> <span class="o">=</span> <span class="n">Titanic</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Age&#39;</span><span class="p">])</span>
<span class="n">Titanic</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(714, 12)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">Titanic</span><span class="p">[[</span><span class="s1">&#39;Age&#39;</span><span class="p">,</span> <span class="s1">&#39;Fare&#39;</span><span class="p">]]</span>
</pre></div>
</div>
</div>
</div>
<p><strong>가우시안 나이브 베이즈</strong></p>
<p>예측변수가 모두 정량적 변수이기 때문에 <strong>가우시안 나이브 베이즈</strong>가 적절하다. 모델을 가우시안 나이브 베이즈(<code class="docutils literal notranslate"><span class="pre">GaussianNB</span></code>)로 선택하고, 모델의 이름을 <code class="docutils literal notranslate"><span class="pre">gnb</span></code>로 지정했다. 그런 다음, <code class="docutils literal notranslate"><span class="pre">fit</span></code> 메서드를 사용하여 모델을 피팅하는데, 이를 위해 예측변수(<code class="docutils literal notranslate"><span class="pre">X</span></code>)와 반응변수(<code class="docutils literal notranslate"><span class="pre">Titanic.Survived</span></code>)를 입력한다. 그 피팅 결과를 바탕으로 <code class="docutils literal notranslate"><span class="pre">predict</span></code> 메서드를 이용해 테스트 세트 관측(<code class="docutils literal notranslate"><span class="pre">X</span></code>)에 대해 클래스를 예측하도록 하여 <code class="docutils literal notranslate"><span class="pre">pred</span></code>라는 이름으로 지정했다.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gnb</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span>
<span class="n">gnb_fit</span> <span class="o">=</span> <span class="n">gnb</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Titanic</span><span class="o">.</span><span class="n">Survived</span><span class="p">)</span>
<span class="n">pred</span> <span class="o">=</span> <span class="n">gnb_fit</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">confusion_matrix()</span></code> 함수를 사용해서 <code class="docutils literal notranslate"><span class="pre">pred</span></code>로 지정해 놓은 예측 클래스를 실제 클래스(<code class="docutils literal notranslate"><span class="pre">Titanic.Survived</span></code>)와 비교하여 혼동행렬을 만들었다. 아래 결과를 보면, 정확도(accuracy)가 65.3%이다.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">Titanic</span><span class="o">.</span><span class="n">Survived</span><span class="p">,</span> <span class="n">pred</span><span class="p">)</span> 
<span class="n">cm_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">cm</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;0&#39;</span><span class="p">,</span><span class="s1">&#39;1&#39;</span><span class="p">],</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;0&#39;</span><span class="p">,</span><span class="s1">&#39;1&#39;</span><span class="p">])</span>
<span class="n">cm_df</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;True&#39;</span>
<span class="n">cm_df</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;Predicted&#39;</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cm_df</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">Titanic</span><span class="o">.</span><span class="n">Survived</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">digits</span><span class="o">=</span><span class="mi">3</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Predicted    0   1
True              
0          399  25
1          223  67

              precision    recall  f1-score   support

           0      0.641     0.941     0.763       424
           1      0.728     0.231     0.351       290

    accuracy                          0.653       714
   macro avg      0.685     0.586     0.557       714
weighted avg      0.677     0.653     0.596       714
</pre></div>
</div>
</div>
</div>
<p><strong>로지스틱 회귀 모델 추정</strong></p>
<p>이번에는 로지스틱 회귀 모델로 동일한 예측변수를 사용하여 생존 확률을 예측하고, 그것에 기반하여 생존 여부를 분류해보자. 반응변수는 생존 더미변수인 <code class="docutils literal notranslate"><span class="pre">Survived</span></code>이고, 예측변수는 <code class="docutils literal notranslate"><span class="pre">Age</span></code> 및 <code class="docutils literal notranslate"><span class="pre">Fare</span></code>이다.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">logit</span><span class="p">(</span><span class="s1">&#39;Survived ~ Age + Fare&#39;</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">Titanic</span><span class="p">)</span>
<span class="n">logitfit</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">logitfit</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Optimization terminated successfully.
         Current function value: 0.624185
         Iterations 6
                           Logit Regression Results                           
==============================================================================
Dep. Variable:               Survived   No. Observations:                  714
Model:                          Logit   Df Residuals:                      711
Method:                           MLE   Df Model:                            2
Date:                Fri, 17 Oct 2025   Pseudo R-squ.:                 0.07587
Time:                        23:58:00   Log-Likelihood:                -445.67
converged:                       True   LL-Null:                       -482.26
Covariance Type:            nonrobust   LLR p-value:                 1.286e-16
==============================================================================
                 coef    std err          z      P&gt;|z|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept     -0.4171      0.186     -2.243      0.025      -0.782      -0.053
Age           -0.0176      0.006     -3.103      0.002      -0.029      -0.006
Fare           0.0173      0.003      6.596      0.000       0.012       0.022
==============================================================================
</pre></div>
</div>
</div>
</div>
<p>아래는 생존 확률 분계점으로 0.5를 사용하여 훈련 세트에 대해 생존 여부를 분류한 것이다. 정확도(accuracy)가 65.7%로 앞의 나이브 베이즈(65.3%)와 큰 차이가 없다.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">logit_pred</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">logitfit</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Titanic</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">)</span><span class="o">.</span><span class="n">map</span><span class="p">({</span><span class="kc">False</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">True</span><span class="p">:</span> <span class="mi">1</span><span class="p">})</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">Titanic</span><span class="o">.</span><span class="n">Survived</span><span class="p">,</span> <span class="n">logit_pred</span><span class="p">)</span>  
<span class="n">cm_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">cm</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">cm_df</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;True&#39;</span>
<span class="n">cm_df</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;Predicted&#39;</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cm_df</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">Titanic</span><span class="o">.</span><span class="n">Survived</span><span class="p">,</span> <span class="n">logit_pred</span><span class="p">,</span> <span class="n">digits</span><span class="o">=</span><span class="mi">3</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Predicted    0   1
True              
0          380  44
1          201  89

              precision    recall  f1-score   support

           0      0.654     0.896     0.756       424
           1      0.669     0.307     0.421       290

    accuracy                          0.657       714
   macro avg      0.662     0.602     0.589       714
weighted avg      0.660     0.657     0.620       714
</pre></div>
</div>
</div>
</div>
</section>
<section id="chatgpt-q-a">
<h3>ChatGPT Q&amp;A<a class="headerlink" href="#chatgpt-q-a" title="Link to this heading">#</a></h3>
<blockquote>
<div><p><strong>Titanic 데이터세트에서 Survived 여부를 Female, Age, Fare 변수를 사용하여 나이브 베이즈 기법으로 예측하는 코드를 만들어줘.</strong></p>
</div></blockquote>
<hr class="docutils" />
<blockquote>
<div><p>Titanic 데이터셋에서 ‘Survived’ 여부를 예측하기 위해 나이브 베이즈(Naive Bayes) 모델을 사용하는 Python 코드를 작성해보겠습니다. 여기서는 <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> 라이브러리를 활용할 것입니다. ‘Female’, ‘Age’, ‘Fare’ 변수를 특성으로 사용할 예정이며, 먼저 필요한 라이브러리와 데이터를 불러온 후, 데이터를 전처리하고, 마지막으로 나이브 베이즈 모델을 학습시키고 예측을 진행합니다.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import pandas as pd  
from sklearn.model_selection import train_test_split  
from sklearn.naive_bayes import GaussianNB  
from sklearn.metrics import accuracy_score  
from sklearn.impute import SimpleImputer

# 데이터 로드 (Titanic 데이터셋의 경로는 상황에 따라 달라질 수 있습니다)  
titanic = pd.read_csv(&#39;titanic.csv&#39;)

# &#39;Female&#39; 컬럼 생성: &#39;Sex&#39; 컬럼을 사용하여 female이면 1, male이면 0으로 설정  
titanic[&#39;Female&#39;] = titanic[&#39;Sex&#39;].map({&#39;female&#39;: 1, &#39;male&#39;: 0})

# 필요한 특성 선택  
features = [&#39;Female&#39;, &#39;Age&#39;, &#39;Fare&#39;]  
X = titanic[features]  
y = titanic[&#39;Survived&#39;]

# 결측치 처리: &#39;Age&#39;와 &#39;Fare&#39;에 대한 결측치를 평균값으로 대체  
imputer = SimpleImputer(strategy=&#39;mean&#39;)  
X_imputed = imputer.fit_transform(X)

# 데이터를 학습 세트와 테스트 세트로 분할  
X_train, X_test, y_train, y_test = train_test_split(X_imputed, y, test_size=0.3, random_state=42)

# 나이브 베이즈 모델 생성 및 학습  
model = GaussianNB()  
model.fit(X_train, y_train)

# 테스트 데이터에 대해 예측  
predictions = model.predict(X_test)

# 예측 정확도 출력  
print(&quot;Accuracy:&quot;, accuracy_score(y_test, predictions))
</pre></div>
</div>
<p>이 코드는 다음과 같은 단계를 포함합니다:</p>
<ol class="arabic simple">
<li><p>필요한 라이브러리와 데이터를 불러옵니다.</p></li>
<li><p>‘Sex’ 컬럼을 기반으로 새로운 이진 ‘Female’ 컬럼을 생성합니다.</p></li>
<li><p>‘Age’와 ‘Fare’ 컬럼의 결측치를 평균값으로 대체합니다.</p></li>
<li><p>데이터를 학습 세트와 테스트 세트로 분할합니다.</p></li>
<li><p>Gaussian Naive Bayes 모델을 생성하고, 학습 데이터에 대해 학습시킵니다.</p></li>
<li><p>테스트 데이터에 대한 예측을 수행하고, 정확도를 계산하여 출력합니다.</p></li>
</ol>
<p>Titanic 데이터셋의 경로는 사용하는 환경이나 데이터셋이 위치한 경로에 따라 달라질 수 있으니, 실제 경로로 수정해 사용해야 합니다.</p>
</div></blockquote>
<p><strong>ChatGPT가 제시한 코드 실행</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="kn">from</span> <span class="nn">sklearn.impute</span> <span class="kn">import</span> <span class="n">SimpleImputer</span>

<span class="c1"># &#39;Female&#39; 컬럼 생성</span>
<span class="n">Titanic</span><span class="p">[</span><span class="s1">&#39;Female&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">Titanic</span><span class="p">[</span><span class="s1">&#39;Sex&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">({</span><span class="s1">&#39;female&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;male&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">})</span>

<span class="c1"># 필요한 특성 선택</span>
<span class="n">features</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Female&#39;</span><span class="p">,</span> <span class="s1">&#39;Age&#39;</span><span class="p">,</span> <span class="s1">&#39;Fare&#39;</span><span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">Titanic</span><span class="p">[</span><span class="n">features</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">Titanic</span><span class="p">[</span><span class="s1">&#39;Survived&#39;</span><span class="p">]</span>

<span class="c1"># 결측치 처리: &#39;Age&#39;와 &#39;Fare&#39;에 대한 결측치를 평균값으로 대체</span>
<span class="n">imputer</span> <span class="o">=</span> <span class="n">SimpleImputer</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">)</span>
<span class="n">X_imputed</span> <span class="o">=</span> <span class="n">imputer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># 데이터를 학습 세트와 테스트 세트로 분할</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X_imputed</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># 나이브 베이즈 모델 생성 및 학습</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># 테스트 데이터에 대해 예측</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># 예측 정확도 출력</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy:&quot;</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predictions</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy: 0.7674418604651163
</pre></div>
</div>
</div>
</div>
</section>
<section id="id19">
<h3>ChatGPT Q&amp;A<a class="headerlink" href="#id19" title="Link to this heading">#</a></h3>
<blockquote>
<div><p><strong>똑같은 데이터세트와 X, y 변수에 대해 로짓 모델로 추정했을 때의 Accuracy를 구해줘.</strong></p>
</div></blockquote>
<hr class="docutils" />
<blockquote>
<div><p>로지스틱 회귀(Logistic Regression) 모델을 사용하여 Titanic 데이터셋에서 ‘Survived’ 여부를 예측하는 Python 코드를 작성하겠습니다. 앞서 나이브 베이즈 모델에서 사용했던 동일한 데이터셋과 특성(‘Female’, ‘Age’, ‘Fare’)을 사용할 것입니다. 이번에는 <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>의 <code class="docutils literal notranslate"><span class="pre">LogisticRegression</span></code> 클래스를 사용합니다. 데이터 전처리 단계는 이전과 동일하며, 모델 학습 후에는 테스트 데이터셋에 대한 정확도를 계산합니다.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import pandas as pd  
from sklearn.model_selection import train_test_split  
from sklearn.linear_model import LogisticRegression  
from sklearn.metrics import accuracy_score  
from sklearn.impute import SimpleImputer

# 데이터 로드  
titanic = pd.read_csv(&#39;titanic.csv&#39;)

# &#39;Female&#39; 컬럼 생성  
titanic[&#39;Female&#39;] = titanic[&#39;Sex&#39;].map({&#39;female&#39;: 1, &#39;male&#39;: 0})

# 필요한 특성 선택  
features = [&#39;Female&#39;, &#39;Age&#39;, &#39;Fare&#39;]  
X = titanic[features]  
y = titanic[&#39;Survived&#39;]

# 결측치 처리  
imputer = SimpleImputer(strategy=&#39;mean&#39;)  
X_imputed = imputer.fit_transform(X)

# 데이터 분할  
X_train, X_test, y_train, y_test = train_test_split(X_imputed, y, test_size=0.3, random_state=42)

# 로지스틱 회귀 모델 생성 및 학습  
model = LogisticRegression()  
model.fit(X_train, y_train)

# 예측 및 정확도 계산  
predictions = model.predict(X_test)  
accuracy = accuracy_score(y_test, predictions)

print(&quot;Accuracy:&quot;, accuracy)
</pre></div>
</div>
<p>이 코드는 앞서 나이브 베이즈 모델과 비슷한 절차를 따르며, 주요 차이점은 모델 생성 부분에서 <code class="docutils literal notranslate"><span class="pre">LogisticRegression</span></code>을 사용한다는 것입니다. 로지스틱 회귀는 이진 분류 문제에 적합한 모델이며, 이 예제에서는 ‘Survived’ 여부를 예측하는 데 사용됩니다. 코드 실행 후에는 테스트 데이터셋에 대한 정확도가 출력됩니다.</p>
<p>Titanic 데이터셋의 위치는 사용 환경에 따라 다를 수 있으니, 실제 데이터 파일이 위치한 경로로 수정하여 사용해야 합니다.</p>
</div></blockquote>
<p><strong>ChatGPT가 제시한 코드 실행</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="kn">from</span> <span class="nn">sklearn.impute</span> <span class="kn">import</span> <span class="n">SimpleImputer</span>

<span class="c1"># &#39;Female&#39; 컬럼 생성</span>
<span class="n">Titanic</span><span class="p">[</span><span class="s1">&#39;Female&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">Titanic</span><span class="p">[</span><span class="s1">&#39;Sex&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">({</span><span class="s1">&#39;female&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;male&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">})</span>

<span class="c1"># 필요한 특성 선택</span>
<span class="n">features</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Female&#39;</span><span class="p">,</span> <span class="s1">&#39;Age&#39;</span><span class="p">,</span> <span class="s1">&#39;Fare&#39;</span><span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">Titanic</span><span class="p">[</span><span class="n">features</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">Titanic</span><span class="p">[</span><span class="s1">&#39;Survived&#39;</span><span class="p">]</span>

<span class="c1"># 결측치 처리</span>
<span class="n">imputer</span> <span class="o">=</span> <span class="n">SimpleImputer</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">)</span>
<span class="n">X_imputed</span> <span class="o">=</span> <span class="n">imputer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># 데이터 분할</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X_imputed</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># 로지스틱 회귀 모델 생성 및 학습</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># 예측 및 정확도 계산</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy:&quot;</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy: 0.7627906976744186
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./notebooks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="08_KNN%EC%9D%84_%EC%9D%B4%EC%9A%A9%ED%95%9C_%EB%B6%84%EB%A5%98.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">8장 KNN 분류</p>
      </div>
    </a>
    <a class="right-next"
       href="10_%EB%B3%80%EC%88%98%EC%84%A0%ED%83%9D_%EB%B0%8F_%EC%A0%95%EA%B7%9C%ED%99%94.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">10장 변수선택 및 정규화</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">9.1 스팸 필터링 예제: 확률론적 접근</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">스팸 필터링 예제 1</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">스팸 필터링 예제 2</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">조건부확률과 베이즈 정리</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">9.2 나이브 베이즈 분류 예제</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">스팸 메일 분류 예제</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#naive">순진한(naive) 가정</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">9.3 나이브 베이즈 알고리즘</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">확률 모델을 이용한 분류기</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">사전적 확률 추정</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">가능도 추정</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">9.4 나이브 베이즈 실습</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">주식시장 데이터</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">나이브 베이즈 분류</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id15">모델별 정확도 비교</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">타이타닉 데이터</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id17">성별에 의한 생존 여부 예측</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id18">나이 및 운임에 의한 생존 여부 예측</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#chatgpt-q-a">ChatGPT Q&amp;A</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id19">ChatGPT Q&amp;A</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By 최필선 민인식
</p>

  </div>
  
  <div class="footer-item">
    

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>